{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecdfae43-28e0-40b6-91c4-1a2ba94035ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "116e99db-f5d4-43f4-9820-d4371d1613ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ephod.utils' from '/Users/jgado/Dropbox/research/projects/ephod/ephod_publish/EpHod/ephod/utils.py'>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import esm\n",
    "import sys, os\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "import importlib\n",
    "from ephod import run, utils\n",
    "\n",
    "importlib.reload(run)\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bec92fd2-2538-496e-bf1d-b4b80137d4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ephod.training.nn_models.ResidualLightAttention"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.models.ResidualLightAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4330872f-7147-4ca4-a6bf-faf7fb53c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jgado/Dropbox/research/projects/ephod/ephod_publish/zenodo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ec5e043-098d-4015-915a-1558426bc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b566bc80-4e85-478e-b85e-5b5552c0dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ephodsvr = joblib.load(f\"{path}/ESM1v-SVR.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf443827-0213-4c6a-acec-72bc37c57919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(gamma=&#x27;auto&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(gamma=&#x27;auto&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR(gamma='auto')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ephodsvr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "431be102-363d-4ac5-be11-eed30195a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ephod.training import nn_models\n",
    "importlib.reload(nn_models)\n",
    "from ephod.training.nn_models import ResidualLightAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4cac595a-7895-4dd1-ab7d-ff9aead7a1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12862.9707, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for p in model.parameters():\n",
    "    total += torch.sum(p)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1198e6b2-a1b6-4f61-a26e-21fd884e2cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResidualLightAttention(dim=1280, kernel_size=7, dropout=0.1, res_blocks=4, activation='elu')\n",
    "model = DataParallel(model)\n",
    "url = 'https://zenodo.org/records/14252615/files/ESM1v-RLATtr.pt?download=1'\n",
    "model_dict = torch.hub.load_state_dict_from_url(url, progress=False, map_location=\"cpu\")\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "695c2543-fc8f-4fb5-9e18-4f4c516b7c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-63.0517, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for p in model.parameters():\n",
    "    total += torch.sum(p)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "14adb66c-22ba-435b-9f46-3fc224a2f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpHodModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        if self.device != 'cuda':\n",
    "            print('WARNING: You are not using a GPU which will be slow.')\n",
    "        self.esm1v_model, self.esm1v_batch_converter = self.load_ESM1v_model()\n",
    "        self.svr_model, self.svr_stats = self.load_SVR_model()\n",
    "        self.rlat_model = self.load_RLAT_model()\n",
    "        _ = self.esm1v_model.eval()\n",
    "        _ = self.rlat_model.eval()\n",
    "        \n",
    "    \n",
    "    def load_ESM1v_model(self):\n",
    "        '''Return pretrained ESM1v model weights and batch converter'''\n",
    "        \n",
    "        model, alphabet = esm.pretrained.esm1v_t33_650M_UR90S_1()\n",
    "        model = model.to(self.device)\n",
    "        batch_converter = alphabet.get_batch_converter()\n",
    "        \n",
    "        return model, batch_converter\n",
    "    \n",
    "    \n",
    "    def get_ESM1v_embeddings(self, accs, seqs):\n",
    "        '''Return per-residue embeddings (padded) for protein sequences from ESM1v model'''\n",
    "\n",
    "        seqs = [utils.replace_noncanonical(seq, 'X') for seq in seqs]\n",
    "        data = [(accs[i], seqs[i]) for i in range(len(accs))]\n",
    "        batch_labels, batch_strs, batch_tokens = self.esm1v_batch_converter(data)\n",
    "        batch_tokens = batch_tokens.to(device=self.device, non_blocking=True)\n",
    "        emb = self.esm1v_model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "        emb = emb[\"representations\"][33]\n",
    "        emb = emb.transpose(2,1) # From (batch, seqlen, features) to (batch, features, seqlen)\n",
    "        emb = emb.to(self.device)\n",
    "\n",
    "        return emb\n",
    "    \n",
    "    \n",
    "    def load_RLAT_model(self):\n",
    "        '''Return residual light attention top model'''\n",
    "        \n",
    "        model = ResidualLightAttention(dim=1280, kernel_size=7, dropout=0.1, res_blocks=4, activation='elu')\n",
    "        model = DataParallel(model)\n",
    "        url = 'https://zenodo.org/records/14252615/files/ESM1v-RLATtr.pt?download=1'\n",
    "        model_dict = torch.hub.load_state_dict_from_url(url, progress=False, map_location=\"cpu\")\n",
    "        model.load_state_dict(model_dict)\n",
    "        model.to(self.device)\n",
    "\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def load_SVR_model(self):\n",
    "        '''Return SVR top model'''\n",
    "        \n",
    "        this_dir, this_filename = os.path.split(__file__)\n",
    "        path = os.path.join(this_dir, 'data', 'ESM1v-SVR.pkl')\n",
    "        svr_model, svr_stats = joblib.load(path)\n",
    "\n",
    "        return svr_model, svr_stats\n",
    "        \n",
    "\n",
    "    \n",
    "    def batch_predict_rlat(self, accs, seqs):\n",
    "        '''Predict pHopt with EpHod on a batch of sequences'''\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Get ESM1v embeddings and run RLATtr model\n",
    "            emb_esm1v = self.get_ESM1v_embeddings(accs, seqs)\n",
    "            maxlen = emb_esm1v.shape[-1]\n",
    "            masks = [[1] * len(seqs[i]) + [0] * (maxlen - len(seqs[i])) \\\n",
    "                     for i in range(len(seqs))]\n",
    "            masks = torch.tensor(masks, dtype=torch.int32)\n",
    "            masks = masks.to(self.device)\n",
    "            out = self.rlat_model(emb_esm1v, masks)\n",
    "            rlat_pred, rlat_emb, rlat_attn = [item.cpu().numpy() for item in out]\n",
    "        \n",
    "            # Run SVR\n",
    "            emb_pool = emb_esm1v.numpy().mean(axis=-1) # (batch, features, seqlen)\n",
    "            emb_pool = (emb_pool - self.svr_stats[0]) / (self.svr_stats[1] + 1e-8) # Normalize\n",
    "            svr_pred = self.svr_model.predict(emb_pool)\n",
    "            ensemble_pred = (rlat_pred + svr_pred) / 2\n",
    "        outdict = dict(rlat_pred=rlat_pred, rlat_emb=rlat_emb, rlat_attn=rlat_attn, \n",
    "                       svr_pred=svr_pred, ensemble_pred=ensemble_pred)\n",
    "\n",
    "        return outdict\n",
    "            \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131cc7d-cb1f-4395-8400-e584d8c6834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ephod_model = EpHodModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1ee7f42b-84a5-4f87-8d42-724c5225be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = ['1', '2']\n",
    "seqs = ['MNTDVRIEKDFLGEKEIPKDAYYGVQTIRATENFPITGYRIHPELIKSLGIVKKSAALANMEVGLLDKEVGQYIVKAADEVIEGKWNDQFIVDPIQGGAGTSINMNANEVIANRALELMGEEKGNYSKISPNSHVNMSQSTNDAFPTATHIAVLSLLNQLIETTKYMQQEFMKKADEFAGVIKMGRTHLQDAVPILLGQEFEAYARVIARDIERIANTRNNLYDINMGATAVGTGLNADPEYISIVTEHLAKFSGHPLRSAQHLVDATQNTDCYTEVSSALKVCMINMSKIANDLRLMASGPRAGLSEIVLPARQPGSSIMPGKVNPVMPEVMNQVAFQVFGNDLTITSASEAGQFELNVMEPVLFFNLIQSISIMTNVFKSFTENCLKGIKANEERMKEYVEKSIGIITAINPHVGYETAAKLAREAYLTGESIRELCIKYGVLTEEQLNEILNPYEMTHPGIAGRK', \n",
    "        'MTAIIDIVGREILDSRGNPTVEVDVVLEDGSFGRAAVPSGASTGAHEAVELRDGGSRYLGKGVEKAVEVVNGKIFDAIAGMDAESQLLIDQTLIDLDGSANKGNLGANAILGVSLAVAKAAAQASGLPLYRYVGGTNAHVLPVPMMNIINGGAHADNPIDFQEFMILPVGATSIREAVRYGSEVFHTLKKRLKDAGHNTNVGDEGGFAPNLKNAQAALDFIMESIEKAGFKPGEDIALGLDCAATEFFKDGNYVYEGERKTRDPKAQAKYLAKLASDYPIVTIEDGMAEDDWEGWKYLTDLIGNKCQLVGDDLFVTNSARLRDGIRLGVANSILVKVNQIGSLSETLDAVETAHKAGYTAVMSHRSGETEDSTIADLAVATNCGQIKTGSLARSDRTAKYNQLIRIEEELGKQARYAGRSALKLL'\n",
    "       ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f1aed3b8-2dc1-49fb-a054-3417e2fe7d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.8687, 7.2128])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = ephod_model.batch_predict(accs, seqs)\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6f5f033d-bc6b-4ba8-ae0b-54de63808f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1280, 470])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d38d733e-3b28-4853-9e1f-2e0fef58cc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ephod.run' from '/Users/jgado/Dropbox/research/projects/ephod/ephod_publish/EpHod/ephod/run.py'>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ephod import run, utils\n",
    "importlib.reload(utils)\n",
    "importlib.reload(run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2a81b819-a0e0-4db0-8e5c-7d2472046721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are not using a GPU which will be slow.\n"
     ]
    }
   ],
   "source": [
    "ephod_model = run.EpHodModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57483696-6cd3-4e63-99f7-8caa776ba0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = ['1', '2']\n",
    "seqs = ['MNTDVRIEKDFLGEKEIPKDAYYGVQTIRATENFPITGYRIHPELIKSLGIVKKSAALANMEVGLLDKEVGQYIVKAADEVIEGKWNDQFIVDPIQGGAGTSINMNANEVIANRALELMGEEKGNYSKISPNSHVNMSQSTNDAFPTATHIAVLSLLNQLIETTKYMQQEFMKKADEFAGVIKMGRTHLQDAVPILLGQEFEAYARVIARDIERIANTRNNLYDINMGATAVGTGLNADPEYISIVTEHLAKFSGHPLRSAQHLVDATQNTDCYTEVSSALKVCMINMSKIANDLRLMASGPRAGLSEIVLPARQPGSSIMPGKVNPVMPEVMNQVAFQVFGNDLTITSASEAGQFELNVMEPVLFFNLIQSISIMTNVFKSFTENCLKGIKANEERMKEYVEKSIGIITAINPHVGYETAAKLAREAYLTGESIRELCIKYGVLTEEQLNEILNPYEMTHPGIAGRK', \n",
    "        'MTAIIDIVGREILDSRGNPTVEVDVVLEDGSFGRAAVPSGASTGAHEAVELRDGGSRYLGKGVEKAVEVVNGKIFDAIAGMDAESQLLIDQTLIDLDGSANKGNLGANAILGVSLAVAKAAAQASGLPLYRYVGGTNAHVLPVPMMNIINGGAHADNPIDFQEFMILPVGATSIREAVRYGSEVFHTLKKRLKDAGHNTNVGDEGGFAPNLKNAQAALDFIMESIEKAGFKPGEDIALGLDCAATEFFKDGNYVYEGERKTRDPKAQAKYLAKLASDYPIVTIEDGMAEDDWEGWKYLTDLIGNKCQLVGDDLFVTNSARLRDGIRLGVANSILVKVNQIGSLSETLDAVETAHKAGYTAVMSHRSGETEDSTIADLAVATNCGQIKTGSLARSDRTAKYNQLIRIEEELGKQARYAGRSALKLL'\n",
    "       ]\n",
    "out = ephod_model.batch_predict(accs, seqs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
